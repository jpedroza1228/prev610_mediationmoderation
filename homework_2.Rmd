---
title: "Homework 2"
author: "JP"
date: "10/25/2020"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r prior functions, echo = FALSE, eval = TRUE}

mcar <- function(x){ 
  if(!require(norm)) {
    stop("You must have norm installed to use LittleMCAR") 
  } 
  
  # if(!require(data.table)) {
  #   stop("Please install the R-package data.table to use mcar")
  # }
  
  if(!(is.matrix(x) | is.data.frame(x))) {
    stop("Data should be a matrix or dataframe")
  }
  
  if (is.data.frame(x)){
    x <- data.matrix(x)
  }
  
  # delete rows of complete missingness
  foo <- function(x) return(any(!is.na(x)))
  dd <- apply(X = x, MARGIN = 1L, FUN = foo)
  dd <- which(!dd, arr.ind = TRUE)
  if(length(dd) > 0) 
    x <- x[-dd,]
  
  # define variables        
  n.var <- ncol(x) # number of variables
  n <- nrow(x)  #number of respondents
  var.names <- colnames(x)
  r <- 1 * is.na(x)
  
  nmis <- as.integer(apply(r, 2, sum))  #number of missing data for each variable REWRITE
  mdp <- (r %*% (2^((1:n.var - 1)))) + 1  #missing data patterns
  x.mp <- data.frame(cbind(x,mdp)) # add column indicating pattern
  colnames(x.mp) <- c(var.names,"MisPat") # set name of new column to MisPat
  n.mis.pat <- length(unique(x.mp$MisPat)) # number of missing data patterns
  p <- n.mis.pat-1 # number of Missing Data patterns minus 1 (complete data row)
  
  
  s <- prelim.norm(x)
  ll <- em.norm(s)
  fit <- getparam.norm(s = s, theta = ll)
  
  # gmean<-mlest(x)$muhat #ML estimate of grand mean (assumes Normal dist)
  gmean <- fit$mu
  # gcov<-mlest(x)$sigmahat #ML estimate of grand covariance (assumes Normal dist)
  gcov <- fit$sigma
  colnames(gcov) <- rownames(gcov) <- colnames(x)
  
  #recode MisPat variable to go from 1 through n.mis.pat
  x.mp$MisPat2 <- rep(NA,n)
  for (i in 1:n.mis.pat){ 
    x.mp$MisPat2[x.mp$MisPat == sort(unique(x.mp$MisPat), partial=(i))[i]]<- i 
  }
  
  x.mp$MisPat<-x.mp$MisPat2
  x.mp<-x.mp[ , -which(names(x.mp) %in% "MisPat2")]
  
  #make list of datasets for each pattern of missing data
  datasets <- list() 
  for (i in 1:n.mis.pat){
    datasets[[paste("DataSet",i,sep="")]]<-x.mp[which(x.mp$MisPat==i),1:n.var]
  }
  
  #degrees of freedom
  kj<-0
  for (i in 1:n.mis.pat){ 
    no.na<-as.matrix(1* !is.na(colSums(datasets[[i]]))) 
    kj<-kj+colSums(no.na) 
  }
  
  df<-kj -n.var
  
  #Little's chi-square
  d2<-0
  cat("this could take a while")
  
  # this crashes at the missingness pattern where every column is missing
  # this for-loop can be handled faster with plyr-function
  for (i in 1:n.mis.pat){ 
    mean <- (colMeans(datasets[[i]])-gmean) 
    mean <- mean[!is.na(mean)] 
    keep <- 1* !is.na(colSums(datasets[[i]])) 
    keep <- keep[which(keep[1:n.var]!=0)] 
    cov <- gcov 
    cov <- cov[which(rownames(cov) %in% names(keep)) , which(colnames(cov) %in% names(keep))] 
    d2 <- as.numeric(d2+(sum(x.mp$MisPat==i)*(t(mean)%*%solve(cov)%*%mean)))
  }
  
  #p-value for chi-square
  p.value<-1-pchisq(d2,df)
  
  #descriptives of missing data
  amount.missing <- matrix(nmis, 1, length(nmis))
  percent.missing <- amount.missing/n
  amount.missing <- rbind(amount.missing,percent.missing)
  colnames(amount.missing) <- var.names
  rownames(amount.missing) <- c("Number Missing", "Percent Missing")
  
  list(chi.square = d2, 
       df = df, 
       p.value = p.value, 
       missing.patterns = n.mis.pat, 
       amount.missing = amount.missing, 
       data = datasets)
}

```


# Homework 1

```{r loading data}

library(tidyverse)
library(lavaan)
library(lavaanPlot)
library(semPlot)
library(MVN)

theme_set(theme_minimal())

data <- read_csv('https://raw.githubusercontent.com/jpedroza1228/prev610_mediationmoderation/1d96144fd9936f4a33ef9c8cf5431de32d5e3572/data/homework2.csv') %>% 
  janitor::clean_names()


# This also works if you followed Dave's resource on creating a R project.

# data <- rio::import(here::here('data', 'homoework2.csv')) %>% 
# janitor::clean_names()
```


```{r subsetted data}

med_sem_data <- data %>% 
  select(cage1,income1,educ1,ftcknow2,harsh3,incons3,pos3,
  ecbin3t,ecbprb3t,sdqpro3)

```

```{r descriptives}
# create univariate histograms

mvn(data = med_sem_data, mvnTest = "mardia", univariateTest = "AD")

hist_fun <- function(data, x){
  ggplot({{data}}, aes({{x}})) +
    geom_histogram(bins = 15, color = 'white',
                   fill = 'dodgerblue')
}

hist_fun(med_sem_data, cage1)

names_col <- c('cage1','income1','educ1','ftcknow2',
               'harsh3','incons3','pos3',
               'ecbin3t','ecbprb3t','sdqpro3')

map2(med_sem_data, names_col, ~hist_fun(med_sem_data, .x) +
      labs(title = glue::glue('Variable: {.y}'))) 
```

```{r missingness}
library(mice)
library(miceadds)

inspectdf::inspect_na(data) %>% 
  inspectdf::show_plot()

par(mfrow=c(1,1)) ##make the plot window one plot per page


md.pattern(med_sem_data)
```

```{r}
# You can use this method for addressing missingness
mcar(med_sem_data)
```

```{r fiml model}
set.seed(574328495) # This is so that the following code will always be the same. Think of this as a save point for your estimates and confidence intervals.

hw_model_1 <- '
# Latent Variables
child_prob =~ ecbin3t + ecbprb3t + sdqpro3

# Regressions
ftcknow2 ~ a1*itt
harsh3 ~ a2*ftcknow2
incons3 ~ a3*ftcknow2
pos3 ~ a4*ftcknow2

child_prob ~ c1*itt + b2*harsh3 + b3*incons3 + b4*pos3 + z1*cage1 + z2*educ1

# Indirect Effects
a1a2b2 := a1*a2*b2
a1a3b3 := a1*a3*b3
a1a4b4 := a1*a4*b4

direct := c1
total := direct + a1a2b2 + a1a3b3 + a1a4b4
'

hw_fit_1 <- sem(hw_model_1,
                data = data,
                estimator = 'mlr',
                missing = 'fiml.x') #The option "ml.x" (alias: "fiml.x" or "direct.x") is similar to "ml", but does not delete any cases with missing values for the exogenous covariates. fiml works here too.

summary(hw_fit_1, standardized = TRUE)
parameterestimates(hw_fit_1)[39:43, ] # the values here are the rows and correspond to the indirect effects

fitmeasures(hw_fit_1)[c('aic', 'bic',
                                 'cfi', 'cfi.robust',
                                 'rmsea', 'rmsea.ci.lower', 'rmsea.ci.upper',
                                 'rmsea.scaled', 'rmsea.ci.lower.scaled', 'rmsea.ci.upper.scaled',
                                 'rmsea.robust', 'rmsea.ci.lower.robust', 'rmsea.ci.upper.robust',
                                 'srmr')]

lavaanPlot(model = hw_fit_1)

lavaanPlot(model = hw_fit_1, stand = TRUE,
           node_options = list(shape = "box",fontname="Helvetica",fontsize="12"),
           edge_options = list(color = "dodgerblue"),
           coefs = TRUE, covs = TRUE,
           stars = c("regress"))

```

```{r boots}
set.seed(574328495)
hw_fit_boot <- sem(model = hw_fit_1, data = data,
              se = "bootstrap", bootstrap = 300,
              estimator = "ml", missing = "fiml.x")                       


summary(hw_fit_boot, standardized = TRUE) 
parameterestimates(hw_fit_1)[39:43,]
```


## Answers

From the model with robust standard errors, it appears that the path from itt to ftcknow2 to pos3 had an indirect effect on child_prob (**p** = .004, 95%CI [-.96, -.19]). From the bootstrapped model, the same path (itt to ftcknow2 to pos3) had a significant indirect effect on child_prob (**p** = .004, 95%CI [-.96, -.19])

Education was the only significant control variable (\beta = .16, **p** = .049). Being so close to the threshold of .50, I would be cautious when interpreting this finding of increased education being associated with more child problems. In the bootstrapped model, the association is no longer significant (\beta = .16, **p** = .057). 


```{r multiple imputed model}

# set.seed(574328495)
# hw_fit_1_mi <- semTools::sem.mi(hw_model_1,
#                 data = data,
#                 m = 5,
#                 miPackage = 'mice')

# summary(hw_fit_1_mi, standardized = TRUE)



# d2 <- fitmeasures(hw_fit_1_mi)[c('aic', 'bic',
#                                  'cfi', 'cfi.robust',
#                                  'rmsea', 'rmsea.ci.lower', 'rmsea.ci.upper',
#                                  'rmsea.scaled', 'rmsea.ci.lower.scaled', 'rmsea.ci.upper.scaled',
#                                  'rmsea.robust', 'rmsea.ci.lower.robust', 'rmsea.ci.upper.robust',
#                                  'srmr')]

# data.frame(d1, d2)

```



